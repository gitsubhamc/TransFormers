{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sinusoidal_positional_embedding(token_sequence_size, token_embedding_dim, n=10000.0):\n",
    "\n",
    "    if token_embedding_dim % 2 != 0:\n",
    "        raise ValueError(\"Sinusoidal positional embedding cannot apply to odd token embedding dim (got dim={:d})\".format(token_embedding_dim))\n",
    "\n",
    "    T = token_sequence_size\n",
    "    d = token_embedding_dim #d_model=head_num*d_k, not d_q, d_k, d_v\n",
    "\n",
    "    positions = torch.arange(0, T).unsqueeze_(1)\n",
    "    print(f\"the positions are -->{positions}--{torch.arange(0, T)}\")\n",
    "    embeddings = torch.zeros(T, d)\n",
    "    print(f\"the embeddings are ---{embeddings}\")\n",
    "\n",
    "    denominators = torch.pow(n, 2*torch.arange(0, d//2)/d) # 10000^(2i/d_model), i is the index of embedding\n",
    "    print(f\"THE denominator is --->{denominators}\")\n",
    "\n",
    "    embeddings[:, 0::2] = torch.sin(positions/denominators) # sin(pos/10000^(2i/d_model))\n",
    "    embeddings[:, 1::2] = torch.cos(positions/denominators) # cos(pos/10000^(2i/d_model))\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the positions are -->tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7]])--tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "the embeddings are ---tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[ 0.00000000,  1.00000000,  0.00000000,  1.00000000],\n",
      "        [ 0.84147096,  0.54030234,  0.00999983,  0.99994999],\n",
      "        [ 0.90929741, -0.41614684,  0.01999867,  0.99980003],\n",
      "        [ 0.14112000, -0.98999250,  0.02999550,  0.99955004],\n",
      "        [-0.75680250, -0.65364361,  0.03998933,  0.99920011],\n",
      "        [-0.95892429,  0.28366220,  0.04997917,  0.99875027],\n",
      "        [-0.27941549,  0.96017027,  0.05996400,  0.99820054],\n",
      "        [ 0.65698659,  0.75390226,  0.06994285,  0.99755102]])\n"
     ]
    }
   ],
   "source": [
    "pos_embds = sinusoidal_positional_embedding(8, 4)\n",
    "torch.set_printoptions(precision=8)\n",
    "print(pos_embds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laama_env",
   "language": "python",
   "name": "laama_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
